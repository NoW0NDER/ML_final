\begin{document}
\title{Report}
\section{Related Work}
\subsection{VADER}
VADER is a rule-based sentiment analysis tool designed for social media texts. It uses lexical and grammatical heuristics to determine sentiment. It has a sentiment lexicon with word scores and considers linguistic features to gauge sentiment intensity. VADER handles negation words and is effective for sentiment analysis in informal text. However, it may struggle with sarcasm, irony, or complex language.
\subsection{FinBERT}
FinBERT is a specialized language model for financial sentiment analysis. It incorporates financial domain-specific knowledge and is trained on financial text, including news articles and social media posts. It can accurately analyze sentiment in the financial domain by capturing contextual features and financial jargon. FinBERT has shown impressive performance in various financial sentiment analysis tasks. However, its effectiveness may be limited outside the financial domain or with non-finance-related text.
\subsection{BERT}
BERT is a powerful language model introduced by Google in 2018. It revolutionized natural language processing (NLP) by capturing contextual information and achieving state-of-the-art performance. BERT is pre-trained on diverse text sources and learns contextual understanding through masked language modeling and next sentence prediction. Its bidirectional nature allows it to effectively capture and utilize contextual information. BERT can be fine-tuned for specific NLP tasks and has been widely used in text classification, named entity recognition, question answering, and sentiment analysis. While BERT has limitations such as computational cost and long-range dependency handling, it has had a significant impact on NLP and paved the way for subsequent models and techniques.
\subsection{RNNs}
Recurrent Neural Networks (RNNs) are neural network architectures designed for sequential data processing. They can capture information from previous steps in a sequence, making them suitable for tasks like natural language processing, speech recognition, and time series analysis. RNNs maintain a hidden state that retains information about the sequence. Traditional RNNs face the vanishing gradient problem, but variants like LSTM and GRU have gating mechanisms to address this issue. RNNs have been applied in various domains, including language modeling, sentiment analysis, machine translation, speech recognition, and time series analysis. However, RNNs can be computationally expensive and may struggle with very long sequences. Despite these limitations, RNNs remain a fundamental architecture in deep learning for sequential data tasks.
\subsubsection{LSTM}
Long Short-Term Memory (LSTM) is a specialized variant of Recurrent Neural Networks (RNNs) designed to address the vanishing gradient problem and capture long-term dependencies in sequential data. LSTMs use gates to selectively retain, forget, and expose information over time. The introduction of a memory cell allows LSTMs to maintain and update information, enabling them to capture complex sequential patterns. LSTMs have been successfully applied in various domains, including natural language processing, speech recognition, machine translation, and time series analysis. They excel in tasks involving long-range dependencies. However, LSTMs can be computationally expensive and require substantial training data. Designing and tuning LSTM architectures can be challenging, and overfitting remains a concern. Nonetheless, LSTMs are widely used and an integral part of many cutting-edge deep learning models for sequential data processing.
\subsubsection{GRU}
Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) that addresses the vanishing gradient problem and captures long-term dependencies in sequential data. GRUs have gained popularity as a simpler alternative to traditional RNNs and have been widely used in various sequential data processing tasks.
Similar to LSTMs, GRUs incorporate gating mechanisms to control the flow of information. However, GRUs have a simpler architecture with fewer gating components. They employ two main gates: the update gate and the reset gate. The update gate determines the amount of information to be updated and stored, while the reset gate controls the influence of past information on the current state. These gates allow GRUs to selectively update and forget information, enabling them to capture long-term dependencies efficiently.
GRUs have been applied successfully in tasks such as natural language processing, speech recognition, recommendation systems, and video analysis. They offer advantages such as simpler architecture, faster training convergence, and lower memory requirements compared to traditional RNNs. GRUs have demonstrated competitive performance and computational efficiency, making them suitable for applications with limited computational resources.
However, the choice between GRUs and LSTMs depends on the specific task and dataset. Empirical evaluation is necessary to determine the most appropriate architecture. Overall, GRUs are a valuable subclass of RNNs that effectively capture long-term dependencies in sequential data, providing an alternative to LSTMs with competitive performance.
\subsubsection{Bidirectional RNNs}
Bidirectional Recurrent Neural Networks (RNNs) capture both past and future contextual information in sequential data by processing the input sequence in both forward and backward directions. They have shown improvements in tasks involving sequential data and are particularly effective when current elements depend on both past and future elements. However, they require the entire sequence upfront and can be computationally expensive.
\subsection{MLP}
The Multilayer Perceptron (MLP) is a widely used type of artificial neural network. It consists of multiple layers of interconnected neurons, including an input layer, one or more hidden layers, and an output layer. MLPs can learn complex relationships in data by using activation functions and adjusting weights through backpropagation. They have been applied to various tasks but may struggle with high-dimensional data and overfitting. Other advanced neural network architectures, like CNNs and RNNs, have gained popularity in certain domains. In summary, the Multilayer Perceptron (MLP) is a versatile and widely used neural network architecture that can capture complex patterns in data. While newer architectures have emerged, MLPs remain a fundamental approach in machine learning and serve as a foundation for more advanced deep learning techniques.

\end{document}