\begin{document}
\title{Report}
\section{Related Work}
\subsection{VADER}
Valence Aware Dictionary and sEntiment Reasoner (VADER) is a rule-based sentiment analysis tool specifically designed for analyzing sentiment in social media texts. Developed to handle the complexities of sentiment analysis in informal and noisy text, VADER employs a combination of lexical and grammatical heuristics to determine the sentiment or emotional tone of a given piece of text.
VADER utilizes a pre-constructed sentiment lexicon containing a comprehensive set of words and their associated sentiment scores. Each word in the lexicon is assigned a polarity score, indicating whether it conveys a positive, negative, or neutral sentiment. By considering linguistic features such as capitalization, punctuation, degree modifiers, and conjunctions, VADER not only identifies sentiment in a sentence but also gauges the intensity of that sentiment, distinguishing between weak and strong sentiments.
One of the notable strengths of VADER is its ability to handle sentiment intensity and context. It recognizes and appropriately accounts for the influence of negation words, ensuring accurate sentiment analysis in negated phrases. This feature makes VADER particularly suitable for sentiment analysis in social media, where informal language, slang, and grammatical errors are prevalent.
VADER has been widely adopted for sentiment analysis tasks, particularly in the analysis of social media data. Its effectiveness in capturing sentiment in informal text and providing sentiment intensity scores has made it a popular choice among researchers and practitioners. However, it is important to note that, like any sentiment analysis tool, VADER has its limitations and may not always accurately capture the intended sentiment, especially in cases involving sarcasm, irony, or complex linguistic structures.
\subsection{FinBERT}
FinBERT is a specialized language model designed for financial sentiment analysis. Based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, FinBERT is specifically trained on a large corpus of financial domain-specific text, including news articles, earnings reports, and social media posts related to finance and investing.
Unlike general-purpose language models, FinBERT incorporates domain-specific knowledge and financial jargon to accurately analyze sentiment in financial text. Its training process involves pre-training on a diverse range of financial data, followed by fine-tuning on specific financial sentiment analysis tasks using labeled data that associates sentiment labels (positive, negative, or neutral) with financial text.
The strength of FinBERT lies in its ability to capture the nuances and contextual features specific to the financial domain. It can extract financial domain-specific information such as company names, stock tickers, and industry-specific terminology, which are crucial for accurate sentiment analysis in the financial industry. FinBERT has demonstrated impressive performance in various financial sentiment analysis tasks, including predicting stock price movements based on news sentiment, analyzing social media sentiment related to financial events, and assessing sentiment in earnings reports.
By leveraging its specialized training and domain-specific knowledge, FinBERT has become a valuable tool for sentiment analysis in the financial domain. However, it is important to note that while FinBERT excels in financial sentiment analysis, it may not perform as effectively in other domains or when faced with text that significantly deviates from finance-related language.
\subsection{BERT}
BERT (Bidirectional Encoder Representations from Transformers) is a powerful language model introduced by Google in 2018. Based on the Transformer architecture, BERT revolutionized natural language processing (NLP) tasks by capturing contextual information and achieving state-of-the-art performance on various benchmarks.
BERT is a pre-trained model that has been trained on a vast amount of text data from diverse sources such as books, articles, and websites. Through its training process, BERT learns to understand the context and meaning of words and sentences by predicting missing words in sentences (masked language modeling) and learning the relationships between different sentences in a text (next sentence prediction).
One of the key features of BERT is its bidirectional nature, which allows it to capture information from both the left and right context of a word. This bidirectional approach enables BERT to capture and utilize contextual information effectively, making it well-suited for a wide range of NLP tasks.
After pre-training, BERT can be fine-tuned on specific NLP tasks such as text classification, named entity recognition, question answering, and sentiment analysis. By adding task-specific layers on top of the pre-trained BERT model and training on task-specific labeled data, BERT can be tailored to different downstream applications.
BERT's contextual understanding and ability to handle complex linguistic relationships have made it a foundational model in NLP research and applications. Its success has paved the way for subsequent models and techniques, and several variants and extensions of BERT, such as RoBERTa, ALBERT, and ELECTRA, have been introduced to further enhance its performance and efficiency in different NLP tasks.
While BERT has achieved remarkable results, it is worth noting that it does have limitations. Its training and inference processes can be computationally expensive, and it may struggle with capturing very long-range dependencies in sequences. Nonetheless, BERT remains a significant advancement in NLP and has had a significant impact on the field.
\subsection{RNNs}
Recurrent Neural Networks (RNNs) are a type of neural network architecture specifically designed for sequential data processing. Unlike traditional feedforward neural networks, RNNs have a recurrent connection that enables them to maintain a hidden state and process sequences of data.
The key advantage of RNNs is their ability to capture and utilize information from previous steps in the sequence to inform the processing of the current step. This makes RNNs well-suited for tasks that involve sequential data, such as natural language processing, speech recognition, time series analysis, and machine translation.
At each step of the sequence, an RNN takes an input and combines it with the hidden state from the previous step to produce an output and update its hidden state. The hidden state serves as the memory of the network, allowing it to retain information about the sequence it has processed so far. The output of an RNN can be used for prediction or can be fed back into the network as input for the next step.
However, traditional RNNs suffer from the vanishing gradient problem, where the gradients used for updating the network's parameters become very small as they propagate back through time. This can make it challenging for RNNs to capture long-range dependencies. To address this issue, various types of RNN variants have been developed, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). These variants introduce gating mechanisms that better control the flow of information and gradients, allowing RNNs to capture and propagate information over longer sequences.
RNNs have been widely used in a range of applications. In natural language processing, they have been applied to tasks such as language modeling, sentiment analysis, and machine translation. In speech recognition, RNNs have been used to process audio signals and convert them into text. In time series analysis, RNNs have been employed for tasks like forecasting and anomaly detection.
While RNNs offer advantages in capturing sequential dependencies, they do have limitations. Training RNNs can be computationally expensive, and they may face challenges in modeling very long sequences. Furthermore, the sequential nature of RNN processing can limit parallelization and affect their efficiency.
Despite these limitations, RNNs remain a fundamental and widely used architecture in the field of deep learning, particularly for tasks involving sequential data.
\subsubsection{LSTM}
Long Short-Term Memory (LSTM) is a specialized variation of Recurrent Neural Networks (RNNs) designed to address the vanishing gradient problem and better capture long-term dependencies in sequential data. LSTMs have gained significant popularity and have become a key component in various tasks involving sequential data processing.
LSTMs were introduced to overcome the limitations of traditional RNNs in capturing and propagating information over long sequences. The core strength of LSTMs lies in their ability to selectively retain and forget information over time using a set of specialized gates. These gates, including the input gate, forget gate, and output gate, control the flow of information through the LSTM unit.
The input gate determines how much new information should be added to the memory, while the forget gate decides which information should be discarded from the memory. The output gate regulates the amount of information that is exposed to the next step in the sequence. By selectively updating and propagating information, LSTMs can effectively capture long-range dependencies and handle complex sequential patterns.
The key innovation of LSTMs is the introduction of the memory cell, which allows the network to maintain and update information over time. The memory cell serves as an internal storage unit that can retain information for long periods, preventing the gradients from vanishing or exploding during the backpropagation process. This enables LSTMs to capture dependencies over extended sequences without suffering from the issues faced by traditional RNNs.
LSTMs have been successfully applied in various domains and tasks, including natural language processing, speech recognition, machine translation, and time series analysis. They have demonstrated superior performance in tasks involving long-range dependencies, such as language modeling, sentiment analysis, and handwriting recognition.
Despite their advantages, LSTMs are not without limitations. They can be computationally expensive and require significant amounts of training data to generalize well. Additionally, designing and tuning LSTM architectures for specific tasks can be challenging, and overfitting remains a concern when the model is excessively complex or the training data is limited.
Nonetheless, LSTMs have become a widely adopted and integral component of many state-of-the-art models in the field of deep learning, offering enhanced capabilities for capturing and modeling sequential data.
\subsubsection{GRU}
Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) that addresses the vanishing gradient problem and improves the modeling of long-term dependencies in sequential data. GRUs have gained popularity as an effective alternative to traditional RNNs and have been widely used in various sequential data processing tasks.
Similar to LSTMs, GRUs are designed to overcome the limitations of standard RNNs in capturing and propagating information over long sequences. GRUs achieve this by incorporating gating mechanisms that control the flow of information through the network. However, compared to LSTMs, GRUs have a simpler architecture with fewer gating components.
GRUs employ two main gates: the update gate and the reset gate. The update gate determines the amount of information to be updated and stored in the hidden state, while the reset gate controls the extent to which the past information influences the current state. These gates allow GRUs to selectively update and forget information, enabling them to capture long-term dependencies while maintaining computational efficiency.
The update gate in GRUs combines the functionality of the input and forget gates in LSTMs, determining how much past information should be retained and how much new information should be incorporated. The reset gate, on the other hand, controls the extent to which the past hidden state influences the computation of the current state.
By using these gating mechanisms, GRUs strike a balance between capturing long-term dependencies and controlling the flow of information. They have been shown to perform well in various tasks involving sequential data, including natural language processing, speech recognition, recommendation systems, and video analysis.
GRUs offer several advantages over traditional RNNs, such as simpler architecture, faster training convergence, and lower memory requirements. They have demonstrated competitive performance compared to LSTMs while being computationally more efficient, making them an attractive choice for applications with limited computational resources.
However, it is worth noting that GRUs may not always outperform LSTMs in all scenarios. The choice between GRUs and LSTMs depends on the specific task and dataset, and empirical evaluation is necessary to determine the most suitable architecture.
In summary, GRUs are a valuable subclass of RNNs that effectively capture long-term dependencies in sequential data. Their simplicity and efficiency make them a popular choice for various sequential data processing tasks, providing an alternative to LSTMs with competitive performance.
\subsubsection{Bidirectional RNNs}
Bidirectional Recurrent Neural Networks (RNNs) are a variation of RNNs that aim to capture both past and future contextual information in sequential data. By processing the input sequence in both forward and backward directions, Bidirectional RNNs enhance the understanding of dependencies and context in both temporal directions.
Traditional RNNs process sequential data in a unidirectional manner, which means they only consider the past context when predicting the current step. However, in many applications, incorporating future context can be equally important. Bidirectional RNNs address this limitation by using two separate hidden states: one that processes the sequence in the forward direction and another that processes it in the backward direction.
During training and inference, the forward hidden state processes the sequence from the beginning to the end, while the backward hidden state processes the sequence in reverse order. The final output is a combination of the information from both directions, typically achieved by concatenating or merging the forward and backward hidden states. This allows Bidirectional RNNs to capture contextual information from both past and future steps, enabling a more comprehensive understanding of the sequence.
Bidirectional RNNs have shown significant improvements in various tasks involving sequential data, including natural language processing, speech recognition, named entity recognition, and sentiment analysis. They are particularly effective in tasks where the current element of the sequence depends on both past and future elements. For example, in speech recognition, Bidirectional RNNs can leverage context from both sides of a phoneme to improve accuracy.
Despite their benefits, Bidirectional RNNs also come with some considerations. They require the entire sequence to be available upfront, making them less suitable for streaming or real-time applications. Additionally, Bidirectional RNNs can be computationally more expensive than unidirectional RNNs due to the need to process the sequence in both directions.
In summary, Bidirectional RNNs offer a valuable extension to traditional unidirectional RNNs by incorporating future context into the modeling of sequential data. Their ability to capture dependencies in both temporal directions has led to improved performance in various tasks. However, the specific choice between Bidirectional RNNs and unidirectional RNNs depends on the nature of the task, the availability of sequence data, and the computational considerations.
\subsection{MLP}
The Multilayer Perceptron (MLP) is a fundamental type of artificial neural network commonly used in machine learning. It is a feedforward neural network consisting of multiple layers of interconnected nodes, known as neurons, organized in a sequential manner.
The MLP architecture comprises an input layer, one or more hidden layers, and an output layer. Each neuron in the network is connected to neurons in the subsequent and preceding layers through weighted connections. The hidden layers serve as intermediate processing layers, transforming the input data into a representation that can be used for making predictions or decisions.
MLPs are capable of learning complex nonlinear relationships in data due to their ability to model high-dimensional and flexible decision boundaries. This is achieved through the use of activation functions, which introduce nonlinearity into the network. Common activation functions used in MLPs include the sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).
Training an MLP involves an iterative process called backpropagation, where the network's weights are adjusted based on the discrepancy between the predicted output and the desired output. This process aims to minimize a predefined loss or error function, such as mean squared error or cross-entropy loss. The optimization is typically performed using gradient descent or its variants.
MLPs have been successfully applied to a wide range of machine learning tasks, including classification, regression, pattern recognition, and function approximation. They are particularly effective in domains where the data has complex relationships and nonlinear dependencies.
However, MLPs also have certain limitations. They may struggle with handling high-dimensional data or large-scale datasets due to the potentially large number of parameters. Overfitting is another concern with MLPs, especially when the model becomes excessively complex or when the training data is limited.
To address these limitations, various techniques have been developed, such as regularization methods (e.g., L1 and L2 regularization), dropout, and early stopping. Additionally, advancements in deep learning, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), have gained prominence and surpassed MLPs in certain domains.
In summary, the Multilayer Perceptron (MLP) is a versatile and widely used neural network architecture that can capture complex patterns in data. While newer architectures have emerged, MLPs remain a fundamental approach in machine learning and serve as a foundation for more advanced deep learning techniques.

\end{document}